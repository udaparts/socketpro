<!DOCTYPE html>
<html lang="en" xmlns="http://www.w3.org/1999/xhtml">
<head>
    <meta charset="utf-8" />
    <title>SocketPro Server Persistent Message Queue</title>
    <link rel="stylesheet" type="text/css" href="codepretty/sons-of-obsidian.css">
    <script type="text/javascript" src="codepretty/prettify.js"></script>
</head>
<body onload="PR.prettyPrint()">
<h1 style="text-align: center;">SocketPro Server Persistent Message Queue</h1>
<p>In previous articles, we have demonstrated SocketPro <a href="cpp_hw.htm" title="SocketPro client server application development">client/server application development</a> and <a href="cpp_push.htm" title="SocketPro Secure Communication and Publish/Subscribe Messaging">subscribe/publish messaging as well as secure communication with industrial standard TLSv1.x protocols</a>.
The two sample projects actually demo the two communication patterns: (1) traditional communication between client and server, and (2) subscribe/publish messaging among connected clients and server.
Further, SocketPro extends the two communication patterns onto web browsers by use of websocket technology, as shown in <a href="cpp_web.htm" title="SocketPro WebSocket for Client/server and Subscribe/publish Messaging on Browsers">this article</a>.</p>
<p>We'll use this article to demonstrate SocketPro server side persistent message queue, which enables that involved parties don't have to interact at the same time with asynchronous communication pattern.
Today, there are many persistent message queue implementations available. Many of them are complicated for reuse or development. In addition, many of them cannot prevent message losses even with lots of your efforts.
However, SocketPro message queue is superiorly fast, and very simple for development, reuse and maintenance. Further, it completely kills message losses with help of SocketPro built-in client (or producer) side queue as shown in <a href="cpp_hw.htm" title="SocketPro client server application development">the previous article</a>.
</p>
<table style="width:100%;">
<tr>
<td style="width:25%;">
<?prettify linenums=1?>
<pre class="prettyprint">
#define TEST_QUEUE_KEY  "queue_name_0"
using namespace std;
typedef CSocketPool&lt;CAsyncQueue&gt; CMyPool;
void TestEnqueue(CMyPool::PHandler &sq);
future&lt;CAsyncQueue::DeqInfo&gt; TestDequeue(CMyPool::PHandler &sq);

int main(int argc, char* argv[]) {
    CConnectionContext cc;
    cc.Port = 20901;
    cc.UserId = L"MyUserId";
    cc.Password = L"MyPassword";
    cout << "Remote host: " << endl;
    getline(cin, cc.Host);
    CMyPool spSq;
    //spSq.SetQueueName("qname");
    if (!spSq.StartSocketPool(cc, 1)) {
        cout << "Failed to connect to remote host for enqueuing" << endl;
        cout << "Press a key to kill the demo ......" << endl;
        ::getchar();
        return 1;
    }
    auto sq = spSq.Seek();//auto sq = spSq.SeekByQueue();
    try{
        TestEnqueue(sq);
        wcout << TestDequeue(sq).get().ToString() << endl;
    }
    catch(CServerError & ex) {
        wcout << ex.ToString() << endl;
    }
    catch(CSocketError & ex) {
        wcout << ex.ToString() << endl;
    }
    catch(exception & ex) {
        wcout << "Some unexpected error: " << ex.what() << endl;
    }
    cout << "Press a key to kill the demo ......" << endl;
    ::getchar();
    return 0;
}
</pre>
</td>
<td>
<p>The client server queue sample project can be found at the directory ../socketpro/tutorials/cplusplus/server_queue/client.
Its main testing code is so simple as shown at the left code snippet 1.</p>
<p>We are going to test enqueuing a number of messages onto a remote SocketPro server queue file at line 24.
Afterwards, we are also going to dequeue these messages back from the remote SocketPro server queue file at line 25.</p>
<p>Now, let's study the two test functions TestEnqueue and TestDequeue as declared at lines 4 and 5, respectively.</p>
</td>
</tr>
</table>
Code snippet 1: Console main for enqueue and dequeue testing at client side
<hr />
<table style="width:100%;">
<tr>
<td style="width:25%;">
<?prettify linenums=1?>
<pre class="prettyprint">
void TestEnqueue(CMyPool::PHandler &sq) {
    cout << "Going to enqueue 1024 msgs ......\n";
    for (int n = 0; n < 1024; ++n) {
        wstring str = to_wstring(n) + L" Object test";
        unsigned short idMessage;
        switch (n % 3) {
            case 0: idMessage = idMessage0;
                break;
            case 1: idMessage = idMessage1;
                break;
            default: idMessage = idMessage2;
                break;
        }
        //this_thread::sleep_for(chrono::milliseconds(100));
        //enqueue two unicode strings and one int
        if (!sq->Enqueue(TEST_QUEUE_KEY, idMessage, str, n)) {
            sq->raise(Queue::idEnqueue);
        }
    }
}
</pre>
</td>
<td>
<p>The left code snippet 2 shows the implementation of the test enqueue function <i>TestEnqueue</i>. Simply speaking, we are going to enqueue 1024 arbitrary messages.
Each of these messages comes with an indentification number <i>idMessage</i> and two unicode strings as well as one integer <i>n</i> as shown at lines 16 and 17.
</p>
<p>Here, the message id <i>idMessage</i> is the same as request id. Later, we use it to identify a message and parse the message when dequeuing.</p>
<p>Additionally, we raise a <i>CSocketError</i> exception by calling the method <i>raise</i> at line 17 if the session or communication channel is closed.</p>
<p>At last, a client is able to open a queue file at remote server by a queue key <i>TEST_QUEUE_KEY</i> as defined at line 1 of the above code snippet 1.
When enqueuing a message, we need to specify a queue by this key as shown at line 16 of the left code snippet 2.</p>
</td>
</tr>
</table>
Code snippet 2: Enqueue a set of messages onto a remote server persistent queue file
<hr />

<table style="width:100%;">
<tr>
<td style="width:30%;">
<?prettify linenums=1?>
<pre class="prettyprint">
future&lt;CAsyncQueue::DeqInfo&gt; TestDequeue(CMyPool::PHandler &sq) {
    //prepare callback for parsing messages dequeued from server side
    sq->ResultReturned = [](CAsyncServiceHandler *sender,
        unsigned short idReq, CUQueue & q) -> bool {
        bool processed = false;
        switch (idReq) {
            case idMessage0:
            case idMessage1:
            case idMessage2:
                cout << "message id=" << idReq;
            {
                wstring name, str; int index;

                //parse a dequeued message which should be the same as the
                //above enqueued message (two unicode strings and one int)
                q >> name >> str >> index;

                wcout << L", name=" << name << L", str=" << str <<
                    L", index=" << index << endl;
            }
                processed = true;
                break;
            default:
                break;
        }
        return processed;
    };
    shared_ptr&lt;promise&lt;CAsyncQueue::DeqInfo&gt; &gt;
            prom(new promise&lt;CAsyncQueue::DeqInfo&gt;);
    auto ab = CAsyncQueue::get_aborted(prom, Queue::idDequeue);
    auto se = CAsyncQueue::get_se(prom);
    //prepare a callback for returned result of dequeue request itself
    CAsyncQueue::DDequeue d = [prom, ab, se](CAsyncQueue*aq, SPA::UINT64
        msg_cnt,SPA::UINT64 fSize,unsigned int msgs,unsigned int bytes) {
        if (bytes) {
            cout << "Total msg count:" << msg_cnt << ", queue file size:"
                << fSize << ", messages dequeued:" << msgs
                << ", message bytes dequeued:" << bytes << endl;
        }
        if (msg_cnt > 0) {
        //If more messages are at server, we re-send a request Dequeue
        aq->Dequeue(TEST_QUEUE_KEY,aq->GetLastDequeueCallback(),0,ab,se);
        } else {
            try {
         prom->set_value(CAsyncQueue::DeqInfo(msg_cnt,fSize,msgs,bytes));
            }
            catch(future_error&) {
                //ignore it
            }
        }
    };
    cout << "Going to dequeue message ......" << endl;
    //Adding one or two extra calls may improve processing concurrency
    //at client and server sides for better performance and throughput
    if (!(sq->Dequeue(TEST_QUEUE_KEY, d, 0, ab, se) &&
        sq->Dequeue(TEST_QUEUE_KEY, d, 0, ab, se))) {
        sq->raise(Queue::idDequeue);
    }
    return prom->get_future();
}
</pre>
</td>
<td>
<p>We are going to investigate the test dequeue function <i>TestDequeue</i> as shown at the left side code snippet 3.
After understanding the piece of code, you will know how to use promise and future within SocketPro for all situations because this piece code is consdered to be the most complex.</p>
<p>First, find the original implementation of the method <i>CAsyncQueue::dequeue</i> at the file ../socketpro/include/aqhandler.h.
Its implementation is very simple as implemetations of other functions returning futures. However, the original implementation of the method <i>dequeue</i> is designed to dequeue one batch of messages only.
This implementation at the snippet 3 is more powerful, which dequeues all messages recursively with more concurrency between client and server sides.</p>
<p>
First of all, we set a callback at line 3 through 27 for parsing messages dequeued from a server queue file. The callback is required to return true or false.
If it returns true, SocketPro adapter knows that the result is proccessed. Otherwise, SocketPro adapter will keep the returning result in the buffer <i>q</i> for possible further processing at client side.
We parse a dequeued message at line 16. It is noted that the data types and parsing orders here must be the same as types and orders at line 17 of the previous code snippet 2.
</p>
<p>
To use modern promise and future for asynchronous requests, first create a shared pointer for promise as shown at lines 28 and 29.
Here, CAsyncQueue::DeqInfo is a structure which will be returned from server in the future.
Afterwards, we prepare a callback <i>ab</i> at line 30 for tracking request aborted event because of either session closed or request canceled.
Next, we prepare a callback <i>se</i> at line 31 for monitoring an exception from server.
The two callbacks are already talked with details at <a href="cpp_hw.htm" title="SocketPro Client Server Application Development">the previous article</a>.</p>
<p>
Next, we prepare a CAsyncQueue::DDequeue callback <i>d</i> at line 33 through 51 to handle returned results of dequeue request itself.
If there are more messages remaining <i>msg_cnt</i> at server queue file, we keep on sending a new request <i>Dequeue</i> recursively at line 42 until remaining messages <i>msg_cnt</i> reach zero.
Otherwise, we set value at line 45.
</p>
<p>
At last, we send two Dequeue requests at lines 55 and 56 with the above three prepared callbacks. Further, keep in mind that messages are dequeued in batch.
To improve dequeuing speed, we call the request Dequeue two times or more for better throughput so that parsing at line 16 client side, dequeuing at server side and data transferring cross network can happen concurrently.
In case a session or communication channel is closed, we raise a <i>CSocketError</i> exception by calling the method <i>raise</i> at line 57.
</p>
<p>Finally, we return a future at line 59 so that a caller is able to wait for a data structure CAsyncQueue::DeqInfo from server side.</p>
</td>
</tr>
</table>
Code snippet 3: Dequeue all messages from a server persistent message queue file
<hr />
<p>It is neccessary to point out that a SocketPro server queue file is sharable by multiple consumers. 
Multiple message producers are able to send messages concurrently into different server queue files at the same time. A server queue is also capable to be consumed or dequeued by different consumers at the same time.</p>
<p>When a message is enqueued from a producer, SocketPro server queue is able to notify all connected consumers so that all connected consumers have a chance to dequeue the message.
Please refer to the memeber <i>MessageQueued</i> of class CAsyncQueue.
When you set a callback at a consumer side, the consumer will be notified if a message is enqueued. Therefore, a consumer doesn't have to poll messages at an interval at all.</p>
<p>This client side demo is very simple. You may refer to its unit test codes at the file ../socketpro/components/uasyncqueue/test_cqueue/test_cqueue.cpp for further studies.</p>
<hr />
<table style="width:100%;">
<tr>
<td style="width:25%;">
<?prettify linenums=1?>
<pre class="prettyprint">
#include "asyncqueueimpl.h"
using namespace SPA::ServerSide;

std::shared_ptr&lt;CSocketProService&lt;CAsyncQueueImpl&gt; &gt; g_pAsyncQueue;
static const unsigned int DEFAULT_DEQUEUE_BATCH_SIZE = 16384;
static const unsigned int MIN_DEQUEUE_BATCH_SIZE = 2048;

bool WINAPI InitServerLibrary(int param) {
    unsigned int options = (unsigned int) param;
    CAsyncQueueImpl::m_bNoAuto = (unsigned char) (options >> 24);
    unsigned int batchSize = (options & 0xffffff);
    if (!batchSize) {
        batchSize = DEFAULT_DEQUEUE_BATCH_SIZE;
    } else if (batchSize < MIN_DEQUEUE_BATCH_SIZE) {
        batchSize = MIN_DEQUEUE_BATCH_SIZE;
    }
    CAsyncQueueImpl::m_nBatchSize = batchSize;
    g_pAsyncQueue.reset(new CSocketProService&lt;CAsyncQueueImpl&gt;
        (SPA::Queue::sidQueue, SPA::taNone));
    return true;
}

void WINAPI UninitServerLibrary() {
    g_pAsyncQueue.reset();
}

unsigned short WINAPI GetNumOfServices() {
    return 1; //The library exposes 1 service only
}

unsigned int WINAPI GetAServiceID(unsigned short index) {
    if (index == 0)
        return SPA::Queue::sidQueue;
    return 0;
}

CSvsContext WINAPI GetOneSvsContext(unsigned int serviceId) {
    CSvsContext sc;
    if (g_pAsyncQueue && serviceId == SPA::Queue::sidQueue)
        sc = g_pAsyncQueue->GetSvsContext();
    else
        memset(&sc, 0, sizeof (sc));
    return sc;
}

unsigned short WINAPI GetNumOfSlowRequests(unsigned int serviceId){
    return 6; //The service only has six slow requests
}

unsigned short WINAPI GetOneSlowRequestID(unsigned int serviceId,
    unsigned short index) {
    //The following six requests are slow ones
    switch (index) {
        case 0: return SPA::Queue::idDequeue;
        case 1: return SPA::Queue::idEnqueue;
        case 2: return SPA::Queue::idFlush;
        case 3: return SPA::Queue::idClose;
        case 4: return SPA::Queue::idEndTrans;
        case 5: return SPA::Queue::idEnqueueBatch;
        default: break;
    }
    return 0;
}
</pre>
</td>
<td>
<p>It is time to move to server side persistent message queue implemetation. Its project is located at the directory ../socketpro/components/uasyncqueue/uasyncqueue.
However, its implemetation code is located at the directory ../socketpro/include/queue/server_impl.</p>
<p>It is noted that SocketPro server side persistent message queue is one of free pre-compiled plugins for you to experiment as shown in <a href="get_started.htm" title="Get Started With SocketPro">this article</a>. When running the test server application <b>all_servers</b>, the plugin will be loaded as long as you distribute it to system directory properly</p>
<p>SocketPro supports server side plugin developments. A plugin is a dynamic shared library written from C/C++ at this time, which is required to export seven functions as defined at the file ../socketpro/include/spa_module.h. As an example, the left code snippet 4 shows the implementation of the seven required functions.</p>
<p><b>InitServerLibrary: </b>This function is the first one to be called by SocketPro server core library (uservercore) when the plugin is loaded into a SocketPro server.
Usually, you may give a parameter (param) for setting global variables as shown at line 10 through 17. Inside the function, we also create an instance of service as shown at lines 18 and 19.</p>
<p><b>GetNumOfServices: </b>The function is the second one to be called after loading the plugin. This plugin exposes one service only. As implied, you can expose multiple services from one plugin.</p>
<p><b>GetAServiceID and GetOneSvsContext: </b>The two functions are the third and fourth ones to be called for service identification numbers from given zero-based indexes.
When SocketPro server core obtains a non-zero service identification number and its service context, the server core will silently register or add the service as shown at <a href="cpp_push.htm" title="SocketPro Secure Communication and Publish/Subscribe Messaging">this previous article</a> or <a href="cpp_web.htm" title="SocketPro WebSocket for Client/server and Subscribe/publish Messaging on Browsers">this previous article</a>.</p>
<p><b>GetNumOfSlowRequests and GetOneSlowRequestID: </b>The two functions are the fifth and sixth ones to be called. The server core will set slow requests by calling the two functions for all exposed services as shown at <a href="cpp_push.htm" title="SocketPro Secure Communication and Publish/Subscribe Messaging">this previous article</a> or <a href="cpp_web.htm" title="SocketPro WebSocket for Client/server and Subscribe/publish Messaging on Browsers">this previous article</a>.</p>
<p><b>UninitServerLibrary: </b>This is the last function may be called when unloading the plugin from server.</p>
<p>At last, the class CAsyncQueueImpl implementation is similar to <a href="cpp_push.htm" title="SocketPro Secure Communication and Publish/Subscribe Messaging">the previous hello world service</a>. As expected, you can figure it out by yourself.
As mentioned before, the source code is at the directory ../socketpro/include/queue/server_impl.
The header file (asyncqueueimpl.h) of the class CAsyncQueueImpl just has 175 or less lines of code. Also, its cpp file (asyncqueueimpl.cpp) has no more than 330 lines of codes. You can easily customerize or extend it for your requirements because of its simplicity.
For example, I could pre-process some of requests from clients before enqueuing them. Also, I can selectively enqueue some of requests, and don't have to enqueue all of them.
</p>
</td>
</tr>
</table>
Code snippet 4: Seven required functions for a SocketPro server plugin
<hr />
<h2>No Message Loss Tests</h2>
<p>Before ending the short article, let's prove SocketPro is capable to prevent message losses easily by use of <a href="cpp_hw.htm" title="SocketPro client server application development">SocketPro client queue</a>.</p>
<p>First, go back to code snippet 1 and uncomment the code at line 15.
Doing so enables SocketPro client (or producer) queue, which will silently back up all requests or messages into a queue file at the speed as fast as possible before sending them to server.
In case network or server down, a client or producer is capable to resend them from its side queue file automatically when network or server is restarted.
Messages or requests will not be lost at all even if a client application is crashed.
</p>
<p>Next, go to the code snippet 2 and uncomment the code at line 14. The code makes enqueuing slow so that you can start and kill the test server <a href="get_started.htm" title="Get Started With SocketPro">all_servers</a> repeatedly.
The code is not neccessary for a real application. It is given here for making tests possible. Otherwise, you have no chance for the test because of its superiorly fast enqueuing and dequeuing.</p>
<p>Start the test <b>all_servers</b> and this console application. You can repeatedly start and kill the demo <b>all_servers</b> during enqueuing.
The final dequeued outputs will be the same and correct as expected. One line of code kills message losses with SocketPro!</p>
<hr />
</body>
</html>
